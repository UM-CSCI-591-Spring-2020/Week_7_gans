{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "basic_gan.ipynb",
      "provenance": [],
      "private_outputs": true,
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDyMMOD24IXo",
        "colab_type": "text"
      },
      "source": [
        "# LET'S GET READY TO RUMBLE!!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CtsBR7DK4IXs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import HTML\n",
        "import matplotlib.animation as animation\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "import torch.functional as F\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as tvtransforms\n",
        "import torchvision.utils as vutils"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2K4d_KD4IXx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Batch size\n",
        "batch_size = 64\n",
        "# number of epochs\n",
        "epochs = 10\n",
        "# learning rate\n",
        "alpha = 0.0002\n",
        "# beta1\n",
        "beta1 = 0.5\n",
        "# beta2\n",
        "beta2 = 0.999\n",
        "# Z vector size\n",
        "nz = 100\n",
        "\n",
        "# Channels, H, W\n",
        "img_shape = (1, 28, 28)\n",
        "# Take advantage of the computer's GPU, if available\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GwUJqF94IX2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.makedirs(\"data/mnist\", exist_ok=True)\n",
        "dataloader = torch.utils.data.DataLoader(\n",
        "    torchvision.datasets.MNIST(\n",
        "        \"data/mnist\",\n",
        "        train=True,\n",
        "        download=True,\n",
        "        transform=tvtransforms.Compose(\n",
        "            [tvtransforms.ToTensor(), tvtransforms.Normalize([0.5], [0.5])]\n",
        "        ),\n",
        "    ),\n",
        "    batch_size=batch_size,\n",
        "    drop_last=True,\n",
        "    num_workers=2,\n",
        "    shuffle=True,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kX46-SXG4IX8",
        "colab_type": "text"
      },
      "source": [
        "First, we need to create our loss functions. From the first paper, the discriminator loss from Algorithm 1, which we want to maximize, is:\n",
        "\n",
        "$\\mathcal{L}_d = \\frac{1}{m}\\sum_{i=1}^m\\left(\\log(D(x^{(i)})) + \\log(1 - D(G(z^{(i)})))\\right)$.\n",
        "\n",
        "The generator loss, which we want to minimize, is\n",
        "\n",
        "$\\mathcal{L}_g = \\frac{1}{m}\\sum_{i=1}^m\\log(1 - D(G(z^{(i)})))$.\n",
        "\n",
        "Unfortunately, this generator loss function can be troublesome in practice. An alternative the authors suggest is\n",
        "\n",
        "$\\mathcal{L}_g = \\frac{1}{m}\\sum_{i=1}^m\\log(D(G(z^{(i)})))$.\n",
        "\n",
        "This function then needs to be maximized."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDIw2FwI4IX9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Some handy functions:\n",
        "# torch.mean()\n",
        "# torch.log()\n",
        "\n",
        "def discriminator_loss(D_of_real, D_of_fake):\n",
        "    # TODO: implment loss for discriminator\n",
        "    # What sign should be applied in order to maximize it?\n",
        "    m = D_of_real.shape[0]\n",
        "    return -1 * torch.mean(torch.log(D_of_real) + torch.log(1 - D_of_fake))\n",
        "def generator_loss(D_of_fake):\n",
        "    # TODO: implment loss for generator\n",
        "    # What sign should be applied in order to maximize or minimize it?\n",
        "    m = D_of_fake.shape[0]\n",
        "    return -1 * torch.mean(torch.log(1 - D_of_fake))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sA99_-iJ4IYC",
        "colab_type": "text"
      },
      "source": [
        "## IN THE G CORNER...\n",
        "A simple generator can be constructed from sequenctial, fully connected layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAG_LFjv4IYD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        \n",
        "        # Calculate the size of the final output vector\n",
        "        flattened_out_size = 1\n",
        "        for dim in img_shape:\n",
        "            flattened_out_size *= dim\n",
        "\n",
        "        # A sequence of layers that are applied to an input\n",
        "        self.model = nn.Sequential(\n",
        "            # Layer 1\n",
        "            # TODO: what should the first network input size be?\n",
        "            nn.Linear(nz, 128),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            \n",
        "            # Layer 2\n",
        "            # Fully connected layer with input size 128, output size 256\n",
        "            nn.Linear(128, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            \n",
        "            # Layer 3\n",
        "            # TODO: add a fully connected layer with input size 256 and output 512\n",
        "            # TODO: add batchnorm\n",
        "            # TODO: add activation\n",
        "            nn.Linear(256, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.LeakyReLU(.2, inplace=True),            \n",
        "            # Layer 4\n",
        "            # TODO: add a fully connected layer with input size 512 and output 1024\n",
        "            # TODO: add batchnorm\n",
        "            # TODO: add activation\n",
        "            nn.Linear(512, 1024),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.LeakyReLU(.2, inplace=True),\n",
        "            # Final Layer\n",
        "            # Map to desired image size\n",
        "            nn.Linear(1024, flattened_out_size),\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "        \n",
        "    def forward(self, z):\n",
        "        # img is flat and needs to be reshaped\n",
        "        img = self.model(z)\n",
        "        # reshape to (batch_size, channels, H, W)\n",
        "        img = img.view(img.size(0), *img_shape)\n",
        "        return img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xtolu1qC4IYI",
        "colab_type": "text"
      },
      "source": [
        "## AND IN THE D CORNER...\n",
        "The descriminator can also be constructed from sequenctial linear layers. We need to down scale the input to a single value for classification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-m1Fn6jc4IYJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        \n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(int(np.prod(img_shape)), 512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            # TODO: add fully connected layer with input size 512 and output size 256\n",
        "            # TODO: add batch norm\n",
        "            # TODO: add activation\n",
        "            nn.Linear(512, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.LeakyReLU(.2, inplace=True),\n",
        "            nn.Linear(256, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "    \n",
        "    def forward(self, img):\n",
        "        # Flatten the image and feed into model\n",
        "        return self.model(img.view(img.size(0), -1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYSIXzP34IYP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create generator and move it to the GPU, if available\n",
        "gen_model = Generator().to(device)\n",
        "print(gen_model)\n",
        "\n",
        "# Create discriminator and move it to the GPU, if available\n",
        "disc_model = Discriminator().to(device)\n",
        "print(disc_model)\n",
        "\n",
        "# Initialize optimizers\n",
        "gen_opt = torch.optim.Adam(gen_model.parameters(), lr=alpha, betas=(beta1, beta2))\n",
        "disc_opt = torch.optim.Adam(disc_model.parameters(), lr=alpha, betas=(beta1, beta2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzIMr27d4IYT",
        "colab_type": "text"
      },
      "source": [
        "# FIGHT!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "hhYNIThy4IYU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "g_losses = []\n",
        "d_losses = []\n",
        "fixed_img_list = []\n",
        "# A fixed set of z vectors to periodically generate images from\n",
        "fixed_z_vecs = torch.randn(64, nz, device=device)\n",
        "iters = 0\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for i, (real_imgs, _) in enumerate(dataloader, 0):\n",
        "        # Move the images to the GPU memory, if possible\n",
        "        real_imgs = real_imgs.to(device)\n",
        "        # Run D on real images\n",
        "        disc_model.zero_grad()\n",
        "        #\n",
        "        # TODO: compute D(x)\n",
        "        #\n",
        "        D_real = disc_model(real_imgs)\n",
        "               \n",
        "        # Run D on generated fake images\n",
        "        z_sample = torch.randn(batch_size, nz, device=device)\n",
        "        #\n",
        "        # TODO: compute G(z)\n",
        "        #\n",
        "        fake_imgs = gen_model(z_sample)\n",
        "        D_fake = disc_model(fake_imgs.detach())\n",
        "\n",
        "        # Compute D's loss and update D's weights\n",
        "        disc_err = discriminator_loss(D_real, D_fake)\n",
        "        disc_err.backward()\n",
        "        disc_opt.step()\n",
        "        \n",
        "        # Update G by maximizing log(D(G(z)))\n",
        "        gen_model.zero_grad()\n",
        "        D_fake = disc_model(fake_imgs)\n",
        "        gen_err = generator_loss(D_fake)\n",
        "        gen_err.backward()\n",
        "        gen_opt.step()\n",
        "        \n",
        "        if i % 100 == 0:\n",
        "            print(\"[{}/{}][{}/{}]\".format(epoch, epochs, i, len(dataloader)) )\n",
        "        if (iters % 500 == 0) or ((epoch == epochs - 1) and (i == len(dataloader) - 1)):\n",
        "            # Periodically capture generated samples using the same z vectors\n",
        "            with torch.no_grad():\n",
        "                fake = gen_model(fixed_z_vecs).detach().cpu()\n",
        "            fixed_img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n",
        "\n",
        "        g_losses.append(gen_err.item())\n",
        "        d_losses.append(disc_err.item())\n",
        "        iters += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIm4nk3x4IYZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "plt.title(\"Generator and Discriminator Loss During Training\")\n",
        "plt.plot(g_losses,label=\"G\")\n",
        "plt.plot(d_losses,label=\"D\")\n",
        "plt.xlabel(\"iterations\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOs7280u4IYe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create slide show from fixed z images\n",
        "fig = plt.figure(figsize=(8, 8))\n",
        "plt.axis(\"off\")\n",
        "ims = [\n",
        "    [plt.imshow(np.transpose(i, (1, 2, 0)), animated=True)]\n",
        "    for i in fixed_img_list\n",
        "]\n",
        "ani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)\n",
        "\n",
        "HTML(ani.to_jshtml())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kvUXbKZ4IYi",
        "colab_type": "text"
      },
      "source": [
        "## Questions\n",
        "1. Does making the generator more complex (adding layers) improve the generated images?\n",
        "2. Does making the discriminator more complex or smarter force the generator to improve as well?\n",
        "3. Can you get the original generator loss function (i.e. log(1 - D(G(z)))) to work? Perhaps transition to it after some number of epochs.\n",
        "4. Bonus: Can you refactor the models to use convolution instead of fully connected layers?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xltJSbuX4IYk",
        "colab_type": "text"
      },
      "source": [
        "## Links\n",
        "* [Link](https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html): pytorch tutorial for DCGAN using a face dataset\n",
        "* [Link](https://junyanz.github.io/CycleGAN/): the CycleGAN website"
      ]
    }
  ]
}